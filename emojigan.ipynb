{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EmojiGAN is a Deep Convolutional Generative Adversarial Network that generates emojis. A perfect companion for AI twitterbots. This model was trained on a GTX 1080ti using Keras with Tensorflow backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.preprocessing import image as k_image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "latent_dim  = 100\n",
    "height = 64\n",
    "width = 64\n",
    "channels = 3\n",
    "depth = 64\n",
    "image_dir = './data/emoji_test_set/*.png'\n",
    "save_dir = './data/emoji_output'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download the \"emoji_imgs_V5\" data set at https://github.com/SHITianhao/emoji-dataset.\n",
    "2. Transfer the emoji .png files into the directory **'/data/emoji_test_set/'**.\n",
    "3. (Optional) Remove any unwanted groups of emojis. E.g., the data used in the model located in **'/models/'** was trained with approximately half the data set omitted. The omitted data were inanimate objects such as flags, buildings, etc. A text file list of the emojis used in the training set can be found in the **'/models/'** directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we set up our model, we must first import and clean up the image data. The emoji .png files we just downloaded are 4-channels of varying height and width. Computer vision usually does not play well with alpha channels, so the function below will convert our 4-channel RGBA images (technically BGRA due to how cv2's imread function operates) to 3-channel RGB images (after converting BGR to RGB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_transparency(source, background_color):\n",
    "    \"\"\"\n",
    "    Converts image from 4-channel image with transparency to a\n",
    "    3-channel image of specified background color.\n",
    "    \"\"\"\n",
    "    \n",
    "    source_img = source[:, :, :3]\n",
    "    source_mask = source[:, :, 3]  * (1 / 255.0)\n",
    "    source_mask = np.repeat(source_mask[:, :, np.newaxis], 3, axis=2)\n",
    "\n",
    "    background_mask = 1.0 - source_mask\n",
    "\n",
    "    bg_part = (background_color * (1 / 255.0)) * (background_mask)\n",
    "    source_part = (source_img * (1 / 255.0)) * (source_mask)\n",
    "\n",
    "    return np.uint8(cv2.addWeighted(bg_part, 255.0, source_part, 255.0, 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below grabs the images in our image directory, adjusts height and width to target dimensions, and replaces transparency with white background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(image_dir, height=64, width=64):\n",
    "    \"\"\"\n",
    "    Compile all images into a single array of \n",
    "    dimensions (n x width x height x 3).\n",
    "    \"\"\"\n",
    "    \n",
    "    image_list = glob.glob(image_dir)\n",
    "    images = []\n",
    "    for image_path in image_list:\n",
    "        img = cv2.imread(image_path, -1)\n",
    "\n",
    "        if img.shape[2] == 4:\n",
    "            img = remove_transparency(img, 255)\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (height, width))\n",
    "\n",
    "        images.append(img)\n",
    "    \n",
    "    images = np.asarray(images) / 255.\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discriminator portion of our GAN model attempts to differentiate between real and generated images. It takes in a candidate image and outputs 0 (real) or 1 (generated). As the discriminator improves, it is better able to detect generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(\n",
    "    height=64, width=64, channels=3, depth=64, lr=0.0001, dropout=0.6, \n",
    "    decay=1e-8):\n",
    "    \"\"\"\n",
    "    Builds discriminator model. The discriminator takes a candidate image\n",
    "    as input and classifies it into two classes: generated or real. Adjust\n",
    "    learning rate and dropout rate to tune performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Conv2D(depth * 1, kernel_size=4,\n",
    "                         input_shape=(height, width, channels), padding=\"same\"))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Conv2D(depth * 1, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2D(depth * 1, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2D(depth * 2, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2D(depth * 4, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Conv2D(depth * 8, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Dropout(dropout))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model_optimizer = keras.optimizers.RMSprop(\n",
    "        lr=lr,\n",
    "        decay=decay,\n",
    "        clipvalue=1.0)\n",
    "\n",
    "    model.compile(optimizer=model_optimizer, loss='binary_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator creates new generated images from inputs of random vectors in latent space. As the generator improves, it becomes better at creating images from these random vectors that could be classified as real images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(\n",
    "    height=64, width=64, channels=3, depth=64, dropout=0.3):\n",
    "    \"\"\"\n",
    "    Builds generator model. Converts a latent space vector into a candidate\n",
    "    image. Kernel density of conv2DTranspose layers is set to a multiple of\n",
    "    stride length to avoid checkerboard artifacts in generated images. Adjust\n",
    "    learning rate (in GAN model) and dropout rate to tune performance. Note\n",
    "    that the generator model is not compiled in this function.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Dense(4 * 4 * depth * 8, input_shape=(latent_dim,)))\n",
    "    model.add(layers.Reshape((4, 4, depth * 8)))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(dropout))\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(depth * 8, kernel_size=4,\n",
    "                                         strides=2, padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(depth * 4, kernel_size=4,\n",
    "                                         strides=2, padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(depth * 2, kernel_size=4,\n",
    "                                         strides=2, padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(depth, kernel_size=4,\n",
    "                                         strides=2, padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2D(channels, kernel_size=7, padding=\"same\"))\n",
    "    model.add(layers.Activation(\"tanh\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GAN model chains the generator to the discriminator. It classifies latent space points as 'fake' or 'real and updates the weight of the generator. Only the generator's weights are updated during GAN training. The discriminator's weights are updated separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The adversarial network\n",
    "def build_gan(latent_dim, generator, discriminator, lr=0.0002, decay=1e-8):\n",
    "    \"\"\"\n",
    "    Build the generative adversarial network (GAN) model. The model chains\n",
    "    the generator and the discriminator. It classifies latent space points\n",
    "    into 'fake' or 'real', and it updates the weight of the generator to \n",
    "    produce images that are more likely to be classified as 'real'. The\n",
    "    weights of the discriminator are frozen during training of the GAN model.\n",
    "    Adjust the learning rate of the generator model here.\n",
    "    \"\"\"\n",
    "    \n",
    "    discriminator.trainable = False\n",
    "    gan_input = keras.Input(shape=(latent_dim,))\n",
    "    gan_output = discriminator(generator(gan_input))\n",
    "    model = keras.models.Model(gan_input, gan_output)\n",
    "\n",
    "    model_optimizer = keras.optimizers.RMSprop(\n",
    "        lr=lr,\n",
    "        decay=decay,\n",
    "        clipvalue=1.0)\n",
    "    \n",
    "    model.compile(optimizer=model_optimizer, loss='binary_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(step, generated_images, real_images, save_dir):\n",
    "    \"\"\"Save sample images during training\"\"\"\n",
    "\n",
    "    img = k_image.array_to_img(generated_images[0] * 255., scale=False)\n",
    "    img.save(os.path.join(save_dir, \n",
    "                         'generated_image' + str(step) + '.png'))\n",
    "\n",
    "    img = k_image.array_to_img(real_images[0] * 255., scale=False)\n",
    "    img.save(os.path.join(save_dir,\n",
    "                         'real_image' + str(step) + '.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator and discriminator are trained in the function below. The function operates as follows:\n",
    "**Discriminator training**\n",
    "1. A batch of random vectors in latent space are generated.\n",
    "2. They are fed through the generator to predict counterfeit images.\n",
    "3. Real images and fake images are combined with corresponding labels.\n",
    "4. The discriminator is trained with these images and targets.\n",
    "\n",
    "**Generator training**\n",
    "5. A new batch of random vectors in latent space are generated.\n",
    "6. These vectors are fed into the GAN with the label \"these are real\"\n",
    "7. Generator weights are adjusted so the discriminator is more likely to mark generated pictures as real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(\n",
    "    discriminator, generator, gan, x_train, iterations, save_dir=None,\n",
    "    batch_size=64, save_intervals=100, save_weights=False):\n",
    "\n",
    "    for step in range(iterations):\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = np.random.normal(size=(batch_size,\n",
    "                                                       latent_dim))\n",
    "        generated_images = generator.predict(random_latent_vectors)\n",
    "\n",
    "        # Retrieve random batch of real images and combine with fake images.\n",
    "        real_image_index = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "        real_images = x_train[real_image_index]\n",
    "        combined_images = np.concatenate([generated_images, real_images])\n",
    "\n",
    "        # Assembles labels, discriminating real from fake images\n",
    "        labels = np.concatenate([np.ones((batch_size, 1)),\n",
    "                                 np.zeros((batch_size, 1))])\n",
    "\n",
    "        # Add random noise to the labels - important!\n",
    "        labels += 0.05 * np.random.random(labels.shape)\n",
    "\n",
    "        # Train the discriminator\n",
    "        d_loss = discriminator.train_on_batch(combined_images, labels)\n",
    "\n",
    "        # Samples random points in space\n",
    "        random_latent_vectors = np.random.normal(size=(batch_size,\n",
    "                                                       latent_dim))\n",
    "\n",
    "        # Assembles labels that say \"these are real images\"\n",
    "        misleading_targets = np.zeros((batch_size, 1))    \n",
    "        a_loss = gan.train_on_batch(random_latent_vectors,\n",
    "                                   misleading_targets)\n",
    "\n",
    "        # occasionally print step and save data   \n",
    "        if step % save_intervals == 0:\n",
    "            print('step:',step)\n",
    "            print('discriminator loss:', d_loss)\n",
    "            print('adversarial loss:', a_loss)\n",
    "            \n",
    "            if save_dir is not None: \n",
    "                save_data(step, generated_images, real_images, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DCGANs are incredibly sensitive to hyperparameter settings, so adjust with caution. The discriminator tends to take over in this system. This can be adjusted by lowering lr_d, increasing lr_g, or increasing discriminator dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters here:\n",
    "iterations = 50000\n",
    "batch_size = 128\n",
    "lr_d = 0.00002 # discriminator learning rate\n",
    "lr_g = 0.0008 # generator learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and initialize models here:\n",
    "x_train = get_images(image_dir)\n",
    "discriminator = build_discriminator(lr=lr_d)\n",
    "generator = build_generator()\n",
    "gan = build_gan(latent_dim, generator, discriminator, lr=lr_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model:\n",
    "train_gan(discriminator, generator, gan, x_train, iterations, save_dir=save_dir,\n",
    "    batch_size=batch_size, save_intervals=100, save_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models:\n",
    "discriminator.save(os.path.join(save_dir,'discriminator.h5'))\n",
    "generator.save(os.path.join(save_dir,'generator.h5'))\n",
    "gan.save(os.path.join(save_dir,'gan.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of generated images at the end.\n",
    "random_latent_vectors = np.random.normal(size=(100, latent_dim))\n",
    "A = generator.predict(random_latent_vectors)\n",
    "A = np.clip(A, 0, 1)\n",
    "fig, axes = plt.subplots(10, 10, figsize=(72, 72),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(A[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
